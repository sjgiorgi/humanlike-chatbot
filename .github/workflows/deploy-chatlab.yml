name: üöÄ Deploy ChatLab Infrastructure (Self-Healing)

on:
  workflow_dispatch:
    inputs:
      db_username:
        description: "Database username (e.g., admin)"
        required: true
        default: "admin"
      db_password:
        description: "Database password"
        required: true
      root_domain:
        description: "Base domain (e.g., chatlab-test.com)"
        required: true
      subdomain:
        description: "Subdomain (e.g., chatlab)"
        required: true
        default: "chatlab"
      force_rebuild:
        description: "Destroy and rebuild all infrastructure?"
        required: true
        default: "false"

permissions:
  contents: read

jobs:
  deploy:
    name: Deploy ChatLab
    runs-on: ubuntu-latest
    timeout-minutes: 60

    env:
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1

    steps:
      - name: üß± Checkout repository
        uses: actions/checkout@v4

      - name: üß© Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.9.5

      - name: üîê Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: ü™£ Ensure Remote Terraform Backend Exists
        run: |
          echo "Checking for S3 bucket and DynamoDB table for Terraform state..."
          aws s3api head-bucket --bucket chatlab-terraform-state 2>/dev/null || \
            aws s3 mb s3://chatlab-terraform-state
          aws dynamodb describe-table --table-name chatlab-locks 2>/dev/null || \
            aws dynamodb create-table \
              --table-name chatlab-locks \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST
          echo "‚úÖ Remote backend is ready."

            - name: üí£ Pre-clean existing AWS resources (safe for reruns)
        run: |
          echo "Cleaning up any previous partial deploys..."

          # IAM Role cleanup
          aws iam get-role --role-name chatlab-eb-ec2-role 2>/dev/null && \
            aws iam delete-role --role-name chatlab-eb-ec2-role || echo "IAM role clean."

          # Instance profile cleanup
          aws iam get-instance-profile --instance-profile-name chatlab-eb-instance-profile 2>/dev/null && \
            aws iam delete-instance-profile --instance-profile-name chatlab-eb-instance-profile || echo "Instance profile clean."

          # RDS subnet group cleanup
          aws rds describe-db-subnet-groups --query "DBSubnetGroups[?DBSubnetGroupName=='chatlab-db-subnet'].DBSubnetGroupName" --output text | \
            grep chatlab-db-subnet && aws rds delete-db-subnet-group --db-subnet-group-name chatlab-db-subnet || echo "RDS subnet clean."

          # CloudFront Origin Access Control cleanup
          aws cloudfront list-origin-access-controls \
            --query "OriginAccessControls[?Name=='ChatLab-OAC'].Id" --output text | \
            xargs -I {} aws cloudfront delete-origin-access-control --id {} || echo "OAC clean."

          # Route53 duplicate CNAME cleanup
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${{ github.event.inputs.root_domain }} --query "HostedZones[0].Id" --output text)
          aws route53 list-resource-record-sets \
            --hosted-zone-id $HOSTED_ZONE_ID \
            --query "ResourceRecordSets[?Name | contains('_26cb8c89a480906084b2ffdfc7469a7b')]" \
            --output json | jq -r '.[].Name' | while read RECORD; do
              aws route53 change-resource-record-sets \
                --hosted-zone-id $HOSTED_ZONE_ID \
                --change-batch "{\"Changes\": [{\"Action\": \"DELETE\", \"ResourceRecordSet\": {\"Name\": \"$RECORD\", \"Type\": \"CNAME\"}}]}" || true
            done

          echo "‚úÖ Cleanup complete."

      - name: ‚öôÔ∏è Initialize Terraform
        working-directory: infra
        run: terraform init -input=false

      - name: üí• Destroy first (if force_rebuild)
        if: ${{ github.event.inputs.force_rebuild == 'true' }}
        working-directory: infra
        run: |
          terraform destroy -auto-approve -input=false \
            -var="aws_region=us-east-1" \
            -var="project_name=chatlab" \
            -var="db_username=${{ github.event.inputs.db_username }}" \
            -var="db_password=${{ github.event.inputs.db_password }}" \
            -var="root_domain=${{ github.event.inputs.root_domain }}" \
            -var="subdomain=${{ github.event.inputs.subdomain }}" || true

      - name: üöÄ Apply Terraform Infrastructure
        working-directory: infra
        run: |
          terraform apply -auto-approve -input=false \
            -var="aws_region=us-east-1" \
            -var="project_name=chatlab" \
            -var="db_username=${{ github.event.inputs.db_username }}" \
            -var="db_password=${{ github.event.inputs.db_password }}" \
            -var="root_domain=${{ github.event.inputs.root_domain }}" \
            -var="subdomain=${{ github.event.inputs.subdomain }}"

      - name: üì¶ Export Terraform Outputs
        working-directory: infra
        run: |
          echo "FRONTEND_BUCKET=$(terraform output -raw frontend_bucket)" >> $GITHUB_ENV
          echo "SITE_URL=$(terraform output -raw site_url)" >> $GITHUB_ENV

      - name: üß∞ Build Frontend
        working-directory: generic_chatbot_frontend
        env:
          CI: false
        run: |
          npm ci
          export REACT_APP_API_URL="${SITE_URL}/api"
          npm run build

      - name: ‚òÅÔ∏è Upload Frontend to S3
        run: |
          aws s3 sync generic_chatbot_frontend/build s3://$FRONTEND_BUCKET --delete
          echo "‚úÖ Deployed frontend to s3://$FRONTEND_BUCKET"
