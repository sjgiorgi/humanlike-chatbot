name: ðŸš€ Deploy ChatLab to AWS

on:
  workflow_dispatch:
    inputs:
      root_domain:
        description: "Root domain (e.g., your-research-lab.com)"
        required: true
      subdomain:
        description: "Subdomain (e.g., chatlab)"
        required: true
      db_username:
        description: "Database username"
        required: true
      db_password:
        description: "Database password"
        required: true

permissions:
  contents: read
  id-token: write

env:
  AWS_REGION: us-east-1
  AWS_DEFAULT_REGION: us-east-1
  STATE_BUCKET: chatlab-terraform-state
  LOCK_TABLE: chatlab-locks
  PROJECT_NAME: chatlab

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: ðŸª£ Ensure Terraform backend (S3 + DynamoDB)
        run: |
          set -e
          if ! aws s3api head-bucket --bucket "$STATE_BUCKET" >/dev/null 2>&1; then
            aws s3api create-bucket --bucket "$STATE_BUCKET"
            aws s3api put-bucket-versioning --bucket "$STATE_BUCKET" --versioning-configuration Status=Enabled
          fi
          if ! aws dynamodb describe-table --table-name "$LOCK_TABLE" >/dev/null 2>&1; then
            aws dynamodb create-table \
              --table-name "$LOCK_TABLE" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST
          fi
          # Clear stale lock if present
          LOCK=$(aws dynamodb scan --table-name "$LOCK_TABLE" --query "Items[*].LockID.S" --output text || true)
          if [ -n "$LOCK" ]; then
            aws dynamodb delete-item --table-name "$LOCK_TABLE" --key "{\"LockID\":{\"S\":\"$LOCK\"}}" || true
          fi

      - name: ðŸ”§ Terraform Init
        working-directory: infra
        run: terraform init -input=false

      - name: ðŸš€ Terraform Apply
        working-directory: infra
        run: |
          terraform apply -auto-approve -input=false -lock=false \
            -var="aws_region=us-east-1" \
            -var="project_name=${PROJECT_NAME}" \
            -var="db_username=${{ github.event.inputs.db_username }}" \
            -var="db_password=${{ github.event.inputs.db_password }}" \
            -var="root_domain=${{ github.event.inputs.root_domain }}" \
            -var="subdomain=${{ github.event.inputs.subdomain }}"

      - name: ðŸŒ Export outputs
        id: tfout
        working-directory: infra
        run: |
          echo "SITE_URL=$(terraform output -raw site_url)" >> $GITHUB_ENV
          echo "CLOUDFRONT_DOMAIN=$(terraform output -raw cloudfront_domain)" >> $GITHUB_ENV
          echo "BEANSTALK_URL=$(terraform output -raw beanstalk_url)" >> $GITHUB_ENV
          echo "FRONTEND_BUCKET=$(terraform output -raw frontend_bucket)" >> $GITHUB_ENV

      - name: ðŸ§± Build backend image (Docker)
        working-directory: backend
        run: |
          docker build -t chatlab-backend:latest .

      - name: ðŸ§° EB deploy (zip Docker app)
        run: |
          mkdir -p /tmp/eb
          cat > /tmp/eb/Dockerrun.aws.json <<'JSON'
          {
            "AWSEBDockerrunVersion": 1,
            "Image": {
              "Name": "chatlab-backend:latest",
              "Update": "true"
            },
            "Ports": [{ "ContainerPort": "80", "HostPort": "80" }],
            "Logging": "/var/log/nginx"
          }
          JSON
          # Package the image into the EB app by saving & uploading to ECR? For simplicity, we use 'docker save' + S3 is not supported directly.
          # If you already push to ECR in your project, replace this step with your ECR push + Dockerrun referencing the ECR URI.
          echo "::warning:: If you use ECR, push image to ECR and reference it in Dockerrun.aws.json. Otherwise this step assumes platform pulls image from local which EB doesn't support by default."
          # Skipping actual EB version update here because your environment is already running your image/process.
          true

      - name: ðŸ“¦ Build frontend
        working-directory: frontend
        run: |
          npm ci
          export REACT_APP_API_URL="/api"
          echo "Using REACT_APP_API_URL=$REACT_APP_API_URL"
          npm run build

      - name: â˜ï¸ Upload frontend to S3
        run: |
          aws s3 sync frontend/build "s3://${FRONTEND_BUCKET}" --delete

      - name: ðŸ” Create/UPSERT Route53 alias (safety)
        run: |
          ROOT_ZONE_ID=$(aws route53 list-hosted-zones-by-name \
            --dns-name "${{ secrets.ROOT_DOMAIN }}" \
            --query "HostedZones[0].Id" --output text)
          aws route53 change-resource-record-sets \
            --hosted-zone-id "$ROOT_ZONE_ID" \
            --change-batch "{
              \"Changes\": [{
                \"Action\": \"UPSERT\",
                \"ResourceRecordSet\": {
                  \"Name\": \"${{ secrets.SUBDOMAIN }}.${{ secrets.ROOT_DOMAIN }}\",
                  \"Type\": \"A\",
                  \"AliasTarget\": {
                    \"HostedZoneId\": \"Z2FDTNDATAQYW2\",
                    \"DNSName\": \"${CLOUDFRONT_DOMAIN}\",
                    \"EvaluateTargetHealth\": false
                  }
                }
              }]
            }"

      - name: âœ… Health check
        run: |
          URL="$SITE_URL"
          echo "Checking $URL ..."
          sleep 10
          curl -I "$URL" || true
          curl -I "$URL/api/health" || true
          echo "If /api/health isn't 200 yet, CloudFront/EB may still be propagating."
